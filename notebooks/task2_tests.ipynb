{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BTAP_ML Phase 3, Task 2 Testing\n",
    "\n",
    "**Description**    \n",
    "This notebook provides the ability to test a suite of potential Multi-Layer Perceptron (MLP) designs on a preprocessed fold of a train/test/validation set, which is provided by the main btap_ml program. The outputs of the training process will be output into a specified output folder for future analysis. Note that some manual edits to the files used may be needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Adjust these based on where the file is run\n",
    "import prepare_weather as pw\n",
    "import preprocessing as pp\n",
    "import feature_selection as fs\n",
    "import predict as pred\n",
    "\n",
    "import csv\n",
    "import datetime\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "from math import sqrt\n",
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import typer\n",
    "from keras import backend as K\n",
    "from keras import regularizers  # for l2 regularization\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras_tuner import Hyperband\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The root folder path of where the preprocessed data is located\n",
    "FILE_PREFIX = ''\n",
    "# Where the preproccesed.json and feature_selection.json files are located in that root folder path\n",
    "preprocessed_data_file = FILE_PREFIX + 'preprocessing/preprocessing.json'\n",
    "selected_features_file = FILE_PREFIX + 'feature_selection/feature_selection.json'\n",
    "# The random seed to be used when testing\n",
    "random_seed = 7\n",
    "# The ability to use parameter tuning (it is recommended to turn this off)\n",
    "param_search = 'no'\n",
    "# The output path from the working directory (can be left blank to have the outputs appear within this working directory)\n",
    "output_path = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Resets all state generated by Keras.\n",
    "K.clear_session()\n",
    "start_time = time.time()\n",
    "# Set the random seeds\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(random_seed)\n",
    "# Load the training, testing, and validation sets\n",
    "with open(preprocessed_data_file, 'r', encoding='UTF-8') as preprocessing_file:\n",
    "    preprocessing_json = json.load(preprocessing_file)\n",
    "# Load the set of features to be used for training\n",
    "with open(selected_features_file, 'r', encoding='UTF-8') as feature_selection_file:\n",
    "    features_json = json.load(feature_selection_file)\n",
    "# Configure the dataframes using the features specified from the preprocessing\n",
    "features = preprocessing_json[\"features\"]\n",
    "selected_features = features_json[\"features\"]\n",
    "X_train = pd.DataFrame(preprocessing_json[\"X_train\"], columns=features)\n",
    "X_test = pd.DataFrame(preprocessing_json[\"X_test\"], columns=features)\n",
    "X_validate = pd.DataFrame(preprocessing_json[\"X_validate\"], columns=features)\n",
    "y_train = pd.read_json(preprocessing_json[\"y_train\"], orient='values').values.ravel()\n",
    "\n",
    "# Extract the selected features from feature engineering\n",
    "X_train = X_train[selected_features]\n",
    "X_test = X_test[selected_features]\n",
    "X_validate = X_validate[selected_features]\n",
    "col_length = X_train.shape[1]\n",
    "\n",
    "# Extract the test data for the target variable\n",
    "y_test_complete = pd.DataFrame(preprocessing_json[\"y_test_complete\"], columns=['energy', 'datapoint_id', 'Total Energy'])\n",
    "y_test = pd.DataFrame(preprocessing_json[\"y_test\"], columns=['energy', 'datapoint_id'])\n",
    "y_validate_complete = pd.DataFrame(preprocessing_json[\"y_validate_complete\"], columns=['energy', 'datapoint_id', 'Total Energy'])\n",
    "y_validate= pd.DataFrame(preprocessing_json[\"y_validate\"], columns=['energy', 'datapoint_id'])\n",
    "\n",
    "# Can combine the train/test/val sets and generate k folds\n",
    "# ...\n",
    "\n",
    "# Scale the data to be used for training and testing\n",
    "scalerx = RobustScaler()\n",
    "scalery = RobustScaler()\n",
    "X_train = scalerx.fit_transform(X_train)\n",
    "X_test = scalerx.transform(X_test)\n",
    "X_validate = scalerx.transform(X_validate)\n",
    "y_train = scalery.fit_transform(y_train.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(os.listdir(\"\")) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The number of epochs to be run (early stopping is enabled by default)\n",
    "EPOCHS = 50\n",
    "# The activation functions to test\n",
    "ACTIVATIONS = ['relu', 'sigmoid']\n",
    "# The dropout rates to test\n",
    "DROPOUT_RATES = [0.1, -1, 0.5]\n",
    "# The learning rates to test\n",
    "LEARNING_RATES = [0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "# Different batch sizes to test\n",
    "BATCH_SIZES = [90, 16, 32, 128, 256]\n",
    "# Layer combinations to test\n",
    "LAYERS = [\n",
    "    [56],\n",
    "    [112],\n",
    "    [1000],\n",
    "    [10000],\n",
    "    [56, 28],\n",
    "    [112, 56],\n",
    "    [1000, 500],\n",
    "    [10000, 5000],\n",
    "    [56, 28, 14],\n",
    "    [112, 56, 28],\n",
    "    [1000, 500, 250],\n",
    "    [10000, 5000, 2500]\n",
    "]\n",
    "# Maintain a counted, starting at 0, to skip tests already performed\n",
    "count = 0\n",
    "# State which test the loop below should resume at, default is 0\n",
    "# Can set to len(os.listdir(\"file_output_directory\")) - 1\n",
    "NUM_FILES_ALREADY_TESTED = 0\n",
    "# Loop through all, or a subset of tests to perform\n",
    "for activation in ACTIVATIONS:\n",
    "    for dropout_rate in DROPOUT_RATES:\n",
    "        for learning_rate in LEARNING_RATES:\n",
    "            for batch_size in BATCH_SIZES:\n",
    "                for layer_design in LAYERS:\n",
    "                    # If the test has not already been tested\n",
    "                    if count > NUM_FILES_ALREADY_TESTED:\n",
    "                        # Set random seeds\n",
    "                        os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"true\"\n",
    "                        os.environ[\"TF_DETERMINISTIC_OPS\"] = \"true\"\n",
    "                        # Resets all state generated by Keras.\n",
    "                        K.clear_session()\n",
    "                        start_time = time.time()\n",
    "                        # Set the random seeds\n",
    "                        np.random.seed(random_seed)\n",
    "                        tf.random.set_seed(random_seed)\n",
    "                        os.environ['PYTHONHASHSEED'] = str(random_seed)\n",
    "                        # Begin the test\n",
    "                        print(\"TEST NUMBER\", count)\n",
    "                        results_pred, hypermodel = pred.create_model(\n",
    "                                        dense_layers=layer_design,\n",
    "                                        activation=activation,\n",
    "                                        optimizer='adam',\n",
    "                                        dropout_rate=dropout_rate,\n",
    "                                        length=col_length,\n",
    "                                        learning_rate=learning_rate,\n",
    "                                        epochs=EPOCHS,\n",
    "                                        batch_size=batch_size,\n",
    "                                        X_train=X_train,\n",
    "                                        y_train=y_train,\n",
    "                                        X_test=X_test,\n",
    "                                        y_test=y_test,\n",
    "                                        y_test_complete=y_test_complete,\n",
    "                                        scalery=scalery,\n",
    "                                        X_validate = X_validate,\n",
    "                                        y_validate=y_validate,\n",
    "                                        y_validate_complete= y_validate_complete,\n",
    "                                        output_path=output_path,\n",
    "                                        path_elec=\"\",\n",
    "                                        path_gas=\"\",\n",
    "                                        val_building_path=\"\"\n",
    "                                       )\n",
    "                        # Also output all training information within one json file\n",
    "                        # Note that the filename maintains all run information, separated by '_'\n",
    "                        filename = \"test_\" + activation + \"_\" + str(learning_rate) + \"_\" + str(batch_size) + \"_\" + str(layer_design) + \"_\" + str(dropout_rate) + \".json\"\n",
    "                        with open(filename, 'w', encoding='utf8') as json_output:\n",
    "                            json.dump(results_pred, json_output)\n",
    "                    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
